{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6adfd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from patsy import dmatrices\n",
    "import scipy.stats as st\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e9f5e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "    \"\"\"\n",
    "    Load the data categorize the columns to categorical and numerical\n",
    "    \"\"\"\n",
    "    def __init__(self, filepath: str = None, targetcol: str = None):\n",
    "        self.data = pd.read_csv(filepath)\n",
    "        self.targetcol = targetcol\n",
    "\n",
    "    def get_numericals(self):\n",
    "        self.num_cols = list(self.data.dtypes[\n",
    "            (self.data.dtypes == 'int64') | (self.data.dtypes == 'float64')| \n",
    "            (self.data.dtypes == 'int32') | (self.data.dtypes == 'float32')|\n",
    "            (self.data.dtypes == 'int16') | (self.data.dtypes == 'float16')|\n",
    "            (self.data.dtypes == 'int8')].keys())\n",
    "        if self.targetcol in self.num_cols:\n",
    "            self.num_cols.remove(self.targetcol)\n",
    "        self.cat_cols = list(set(self.data.columns).difference(self.num_cols))\n",
    "        if self.targetcol in self.cat_cols:\n",
    "            self.cat_cols.remove(self.targetcol)\n",
    "        return self.num_cols, self.cat_cols\n",
    "\n",
    "    def get_inputs(self):\n",
    "        num_cols, cat_cols = self.get_numericals()\n",
    "        return num_cols, self.targetcol, self.data, cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f91f9ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vif(targetcol, data, num_cols, cat_cols):\n",
    "    \"\"\"\n",
    "    Calculate the VIF and get the columns having multicollinear features (high VIF).\n",
    "    args:\n",
    "        targetcol: target column name\n",
    "        data: whole cleaned dataframe\n",
    "        num_cols: List of numerical column names\n",
    "        cat_cols: List of categorical column names.\n",
    "    return:\n",
    "        vif_high: list of features having high VIF>10\n",
    "    \"\"\"\n",
    "    y, X = dmatrices(targetcol+'~'+'+'.join(num_cols+cat_cols), data=data, return_type='dataframe')\n",
    "    vif = pd.DataFrame()\n",
    "    vif[\"VIF Factor\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "    vif[\"features\"] = X.columns\n",
    "    vif_high = list(vif[vif['VIF Factor']>10]['features'])\n",
    "    if 'Intercept' in vif_high:\n",
    "        vif_high.remove('Intercept')\n",
    "    vif_high = list(set([re.sub(r'\\[.*', '', s) for s in vif_high]))\n",
    "    return vif_high\n",
    "\n",
    "\n",
    "def removal_on_vif(targetcol:str=None, data:pd.DataFrame=None, num_cols:list=None, cat_cols:list=None):\n",
    "    \"\"\"\n",
    "    Remove all the features having high VIF.\n",
    "    args:\n",
    "        targetcol: target column name\n",
    "        data: whole cleaned dataframe\n",
    "        num_cols: List of numerical column names\n",
    "        cat_cols: List of categorical column names\n",
    "    return: \n",
    "        num_cols: list containing numerical column names after removal based on VIF\n",
    "        cat_cols: list containing categorical column names after removal based on VIF\n",
    "    \"\"\"\n",
    "    vif_first = vif(targetcol, data, num_cols, cat_cols)\n",
    "    print(\"vif first:\",vif_first)\n",
    "    feat_removal=[]\n",
    "    for i in vif_first:\n",
    "        cols = list(set(num_cols).difference(vif_first))\n",
    "        cols.append(i)\n",
    "        cols1 = list(set(cat_cols).difference(vif_first))\n",
    "        ans = vif(targetcol,data[cols+cols1+[targetcol]],cols,list(set(cat_cols).difference(vif_first)))\n",
    "        for j in ans:\n",
    "            if j in vif_first:\n",
    "                feat_removal.append(j)\n",
    "    feat_removal = list(set([re.sub(r'\\[.*', '', s) for s in feat_removal]))\n",
    "    num_cols = list(set(num_cols).difference(feat_removal))\n",
    "    cat_cols = list(set(cat_cols).difference(feat_removal))\n",
    "    print(f\"Columns Removed on basis of VIF: {feat_removal}\")\n",
    "    return num_cols, cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bec1e811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removal_on_pvalue(targetcol: str = None, num_cols: list = None, cat_cols: list = None, data:pd.DataFrame=None):\n",
    "    \"\"\"\n",
    "        Fit model and remove the columns based on pvalue test on each feature.\n",
    "    args:\n",
    "        targetcol: target column name\n",
    "        data: whole cleaned dataframe\n",
    "        num_cols: List of numerical column names\n",
    "        cat_cols: List of categorical column names\n",
    "    \"\"\"\n",
    "    model = smf.ols(targetcol+'~'+'+'.join(num_cols+cat_cols),data=data).fit()\n",
    "    pvalue_removal = list(model.pvalues[model.pvalues>0.05].keys())\n",
    "    if 'Intercept' in pvalue_removal:\n",
    "        pvalue_removal.remove(\"Intercept\")\n",
    "    pvalue_removal = list(set([re.sub(r'\\[.*', '', s) for s in pvalue_removal]))\n",
    "    num_cols = list(set(num_cols).difference(pvalue_removal))\n",
    "    cat_cols = list(set(cat_cols).difference(pvalue_removal))\n",
    "    print(\"Columns removed on basis of Pvalue:\", pvalue_removal)\n",
    "    return num_cols, cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41882c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def anovatyp1_removal(targetcol: str = None, num_cols: list = None, cat_cols: list = None, data:pd.DataFrame=None):\n",
    "    \"\"\"\n",
    "        Fit anova test and remove the columns failing the anova type 1 test.\n",
    "    args:\n",
    "        targetcol: target column name\n",
    "        data: whole cleaned dataframe\n",
    "        num_cols: List of numerical column names\n",
    "        cat_cols: List of categorical column names\n",
    "        \"\"\"\n",
    "    model = smf.ols(targetcol+'~'+'+'.join(num_cols+cat_cols),data=data).fit()\n",
    "    anova_typ1 = sm.stats.anova_lm(model, typ=1)\n",
    "    m = anova_typ1['PR(>F)']\n",
    "    anovatyp1_removal = list(m[m>0.05].index)\n",
    "    print(\"columns removed on basis of anova typ1:\",anovatyp1_removal)\n",
    "    num_cols = list(set(num_cols).difference(anovatyp1_removal))\n",
    "    cat_cols = list(set(cat_cols).difference(anovatyp1_removal))\n",
    "    return num_cols, cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11408a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalmodel(targetcol: str = None, num_cols: list = None, cat_cols: list = None, data:pd.DataFrame=None):\n",
    "    \"\"\"\n",
    "    Fit final OLS model on data and also get explainability based on tstat values.\n",
    "    args:\n",
    "        targetcol: target column name\n",
    "        data: whole cleaned dataframe\n",
    "        num_cols: List of numerical column names\n",
    "        cat_cols: List of categorical column names\n",
    "    \"\"\"\n",
    "    print(f\"Final Model:{targetcol}+'~'+{'+'.join(num_cols+cat_cols)}\")\n",
    "    num_cols, cat_cols = removal_on_pvalue(targetcol, num_cols, cat_cols, data)\n",
    "    model = smf.ols(targetcol+'~'+'+'.join(num_cols+cat_cols),data=data).fit()\n",
    "    explanation = np.abs(model.tvalues).sort_values(ascending=False)\n",
    "    sns.regplot(x = data[targetcol], y = model.fittedvalues)\n",
    "    return model, explanation, num_cols, cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcb55ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjusted_r2(predictions:list=None, data:str=pd.DataFrame, targetcol:str=None):\n",
    "    \"\"\"\n",
    "    Calculate adjusted R2.\n",
    "    args:\n",
    "        predictions: list of all the fitted model prediction values.\n",
    "        data: DataFrame of initial dataset containing all columns.\n",
    "        targetcol: target column name.\n",
    "    \"\"\"\n",
    "#     try:\n",
    "    r2 = r2_score(predictions,data[targetcol])\n",
    "    adjr2 = 1-(1-r2)*(len(data)-1)/(len(data)-len(data.columns)-1)\n",
    "    return adjr2\n",
    "#     except:\n",
    "#         print(\"Unable to calculate adjusted R2, please check predictions list.\")\n",
    "    \n",
    "\n",
    "def bootstrap_confidence(n_iterations, targetcol, num_cols, cat_cols, data):\n",
    "    \"\"\"\n",
    "    get confidence score of the model using bootstrapping.\n",
    "    args:\n",
    "        targetcol: target column name\n",
    "        data: whole cleaned dataframe\n",
    "        num_cols: List of numerical column names\n",
    "        cat_cols: List of categorical column names\n",
    "        n_iterations: number of iteration to run bootstrapping\n",
    "    return:\n",
    "        Confidence interval: Lower and upper quantile.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        stats = list()\n",
    "        for i in range(n_iterations):\n",
    "            # prepare train and test sets\n",
    "            train, test, y_train, y_test = train_test_split(data[num_cols+cat_cols], data[targetcol], test_size=0.2)    \n",
    "            train[targetcol] = y_train\n",
    "            test[targetcol] = y_test\n",
    "            # fit model\n",
    "            model = smf.ols(targetcol+'~'+'+'.join(num_cols+cat_cols),data=train).fit()\n",
    "            # evaluate model\n",
    "            predictions = model.predict(test[num_cols+cat_cols])\n",
    "            score = adjusted_r2(predictions,test, targetcol)\n",
    "            if i%100 == 0:\n",
    "                print(f\"Adj R2 for iter {i}: {np.round(score,3)*100}%\")\n",
    "            stats.append(score)\n",
    "        alpha = 0.95\n",
    "        p = ((1.0-alpha)/2.0) * 100\n",
    "        lower = max(0.0, np.percentile(stats, p))\n",
    "        p = (alpha+((1.0-alpha)/2.0)) * 100\n",
    "        upper = min(1.0, np.percentile(stats, p))\n",
    "        print('%.1f%% confidence interval %.1f%% and %.1f%%'%(alpha*100, lower*100, upper*100))\n",
    "        return lower*100, upper*100\n",
    "    except:\n",
    "        print(\"not enough resampled data\")\n",
    "        return 0,0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f63b410d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ci_pi(targetcol, num_cols, cat_cols, data):\n",
    "    \"\"\"\n",
    "    Calculate Confidence Interval and Prediction Interval for a given Sample\n",
    "    args:\n",
    "        targetcol: target column name.\n",
    "        num_cols: list of numerical columns.\n",
    "        cat_cols: list of categorical columns.\n",
    "        data: DataFrame of whole initial dataset. \n",
    "    return:\n",
    "        2 tuples Confidence Interval and Prediction Interval\n",
    "    \"\"\"\n",
    "    y, X = dmatrices(targetcol+'~'+'+'.join(num_cols+cat_cols), data=data, return_type='dataframe')\n",
    "    stderr = np.sqrt(mse*np.dot(X.iloc[0].values,np.dot(np.linalg.inv(np.dot(X.T,X)),X.iloc[0].values.T)))\n",
    "    t_conf = st.t.ppf(0.025,len(X)-len(X.columns))\n",
    "    y_hat = model.predict(pd.DataFrame(data[num_cols+cat_cols].iloc[0:3]))[0]\n",
    "    conf_interval = y_hat-(np.abs(t_conf)*stderr), y_hat+(np.abs(t_conf)*stderr)\n",
    "    pred_interval = y_hat-(np.abs(t_conf)*np.sqrt((stderr**2)+mse)), y_hat+(np.abs(t_conf)*np.sqrt((stderr**2)+mse))\n",
    "    return conf_interval, pred_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9f05eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Still left\n",
    "#influential points is left out\n",
    "#Model Assumptions\n",
    "#if anyone vif is high remove all the dummy column and origin base column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d8f598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 201 entries, 0 to 200\n",
      "Data columns (total 72 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   symboling                201 non-null    int64  \n",
      " 1   wheel_base               201 non-null    float64\n",
      " 2   curb_weight              201 non-null    int64  \n",
      " 3   engine_size              201 non-null    int64  \n",
      " 4   bore                     201 non-null    float64\n",
      " 5   stroke                   201 non-null    float64\n",
      " 6   compression_ratio        201 non-null    float64\n",
      " 7   horsepower               201 non-null    float64\n",
      " 8   peak_rpm                 201 non-null    float64\n",
      " 9   city_mpg                 201 non-null    int64  \n",
      " 10  highway_mpg              201 non-null    int64  \n",
      " 11  price                    201 non-null    float64\n",
      " 12  make_alfa_romero         201 non-null    int64  \n",
      " 13  make_audi                201 non-null    int64  \n",
      " 14  make_bmw                 201 non-null    int64  \n",
      " 15  make_chevrolet           201 non-null    int64  \n",
      " 16  make_dodge               201 non-null    int64  \n",
      " 17  make_honda               201 non-null    int64  \n",
      " 18  make_isuzu               201 non-null    int64  \n",
      " 19  make_jaguar              201 non-null    int64  \n",
      " 20  make_mazda               201 non-null    int64  \n",
      " 21  make_mercedes_benz       201 non-null    int64  \n",
      " 22  make_mercury             201 non-null    int64  \n",
      " 23  make_mitsubishi          201 non-null    int64  \n",
      " 24  make_nissan              201 non-null    int64  \n",
      " 25  make_peugot              201 non-null    int64  \n",
      " 26  make_plymouth            201 non-null    int64  \n",
      " 27  make_porsche             201 non-null    int64  \n",
      " 28  make_renault             201 non-null    int64  \n",
      " 29  make_saab                201 non-null    int64  \n",
      " 30  make_subaru              201 non-null    int64  \n",
      " 31  make_toyota              201 non-null    int64  \n",
      " 32  make_volkswagen          201 non-null    int64  \n",
      " 33  make_volvo               201 non-null    int64  \n",
      " 34  fuel_type_diesel         201 non-null    int64  \n",
      " 35  fuel_type_gas            201 non-null    int64  \n",
      " 36  aspiration_std           201 non-null    int64  \n",
      " 37  aspiration_turbo         201 non-null    int64  \n",
      " 38  num_of_doors_four        201 non-null    int64  \n",
      " 39  num_of_doors_two         201 non-null    int64  \n",
      " 40  body_style_convertible   201 non-null    int64  \n",
      " 41  body_style_hardtop       201 non-null    int64  \n",
      " 42  body_style_hatchback     201 non-null    int64  \n",
      " 43  body_style_sedan         201 non-null    int64  \n",
      " 44  body_style_wagon         201 non-null    int64  \n",
      " 45  drive_wheels_4wd         201 non-null    int64  \n",
      " 46  drive_wheels_fwd         201 non-null    int64  \n",
      " 47  drive_wheels_rwd         201 non-null    int64  \n",
      " 48  engine_location_front    201 non-null    int64  \n",
      " 49  engine_location_rear     201 non-null    int64  \n",
      " 50  engine_type_dohc         201 non-null    int64  \n",
      " 51  engine_type_l            201 non-null    int64  \n",
      " 52  engine_type_ohc          201 non-null    int64  \n",
      " 53  engine_type_ohcf         201 non-null    int64  \n",
      " 54  engine_type_ohcv         201 non-null    int64  \n",
      " 55  engine_type_rotor        201 non-null    int64  \n",
      " 56  num_of_cylinders_eight   201 non-null    int64  \n",
      " 57  num_of_cylinders_five    201 non-null    int64  \n",
      " 58  num_of_cylinders_four    201 non-null    int64  \n",
      " 59  num_of_cylinders_six     201 non-null    int64  \n",
      " 60  num_of_cylinders_three   201 non-null    int64  \n",
      " 61  num_of_cylinders_twelve  201 non-null    int64  \n",
      " 62  num_of_cylinders_two     201 non-null    int64  \n",
      " 63  fuel_system_1bbl         201 non-null    int64  \n",
      " 64  fuel_system_2bbl         201 non-null    int64  \n",
      " 65  fuel_system_4bbl         201 non-null    int64  \n",
      " 66  fuel_system_idi          201 non-null    int64  \n",
      " 67  fuel_system_mfi          201 non-null    int64  \n",
      " 68  fuel_system_mpfi         201 non-null    int64  \n",
      " 69  fuel_system_spdi         201 non-null    int64  \n",
      " 70  fuel_system_spfi         201 non-null    int64  \n",
      " 71  volume                   201 non-null    float64\n",
      "dtypes: float64(8), int64(64)\n",
      "memory usage: 113.2 KB\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    data_obj = Data(\"cleaned_data_project1.csv\",\"price\")\n",
    "    num_cols, targetcol, data, cat_cols = data_obj.get_inputs()\n",
    "    data.info()\n",
    "    model = smf.ols(targetcol+'~'+'+'.join(num_cols+cat_cols),data=data).fit()\n",
    "    #print(model.summary())\n",
    "    sns.regplot(data[targetcol],model.fittedvalues)\n",
    "    num_cols, cat_cols = removal_on_vif(targetcol, data, num_cols, cat_cols)\n",
    "    num_cols, cat_cols = removal_on_pvalue(targetcol, num_cols, cat_cols, data)\n",
    "    num_cols, cat_cols = anovatyp1_removal(targetcol,num_cols,cat_cols,data)\n",
    "    model, explanation, num_cols, cat_cols = finalmodel(targetcol,num_cols, cat_cols, data)\n",
    "    fig, ax = plt.subplots(figsize=(10,5))\n",
    "    sns.barplot(explanation.values,explanation.index)\n",
    "    plt.show()\n",
    "    print(model.summary())\n",
    "    mse = model.mse_resid\n",
    "    msr = model.mse_model\n",
    "    print(\"R2 of the Final Model\", model.rsquared_adj)\n",
    "    n_iterations = 1000\n",
    "    lower, upper = bootstrap_confidence(n_iterations, targetcol, num_cols, cat_cols, data)\n",
    "    conf_interval, pred_interval = ci_pi(targetcol, num_cols, cat_cols,data)\n",
    "    print(\"Confidence Interval for a given sample is:\",conf_interval)\n",
    "    print(\"prediction Interval for given sample is:\",pred_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c5bf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418538f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
